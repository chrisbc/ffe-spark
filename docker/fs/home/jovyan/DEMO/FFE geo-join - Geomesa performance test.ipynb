{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FFE geo-join - Geomesa performance test\n",
    "\n",
    "This geo-join operation creates a set of edges for buildings where the distance between centers < N. \n",
    "\n",
    "Although the final solution requires more accurate distance between buildings this first pass will drastically reduce the number of objects that ultimately need to be handled with a more detailed approach.\n",
    "\n",
    "This is a very costly O(n2) operation withoout some geospatial indexing. This is where Spark + Geomesa can help.\n",
    "\n",
    "Spark prvides the parallisation framework, allowing all the available CPU to be used.\n",
    "\n",
    "Geomesa provides Spark-compatible geo-function that use geo-indexing to greatly reduce compute costs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "import geomesa_pyspark\n",
    "\n",
    "from pyspark.sql import Row\n",
    "conf = geomesa_pyspark.configure().setAppName('Demo1')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.enableHiveSupport().getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# geomesa_pyspark.init_sql(spark) in later version 2.4 DOCS)\n",
    "spark._jvm.org.apache.spark.sql.SQLTypes.init(spark._jwrapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"file:///home/jovyan/DEMO/buildings_raw.csv\"\n",
    "\n",
    "df = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = df.repartition(72)\n",
    "newdf.createOrReplaceTempView('df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 16.5 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def timed_join(sample_size=1000):\n",
    "    t0 = dt.utcnow()\n",
    "    qry = \"SELECT df1._c0 as id, \" +\\\n",
    "        \"df2._c0 as near_id \" +\\\n",
    "        \"FROM df as df1, df as df2 \" +\\\n",
    "        \"WHERE st_distance(st_point(df1.X, df1.Y), st_point(df2.X, df2.Y)) < 50 \"+\\\n",
    "        \"AND df1._c0 <> df2._c0 \" +\\\n",
    "        \"AND df1._c0 < %s and df2._c0 < %s\" % (sample_size, sample_size)\n",
    "    result = spark.sql(qry)\n",
    "    count = result.count()\n",
    "    print(\"join + count %s took %s with %s edges\" % (sample_size, dt.utcnow()-t0, count))\n",
    "    # print('ratio', result.count()/(df.count() **2))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join + count 1000 took 0:00:11.302566 with 3966 edges\n",
      "join + count 2000 took 0:00:02.761756 with 14148 edges\n",
      "join + count 4000 took 0:00:06.073014 with 37052 edges\n",
      "+----+-------+\n",
      "|  id|near_id|\n",
      "+----+-------+\n",
      "|2909|   2905|\n",
      "|2909|   2899|\n",
      "|2909|   2900|\n",
      "|2909|   2895|\n",
      "|2909|   2902|\n",
      "|2909|   2903|\n",
      "|2909|   2898|\n",
      "|2909|   2872|\n",
      "|2909|   2904|\n",
      "|2909|   2901|\n",
      "|2355|   1977|\n",
      "|2355|   2346|\n",
      "|2355|   2347|\n",
      "|2041|   2051|\n",
      "|2041|   2396|\n",
      "|2041|   2018|\n",
      "|2041|   2042|\n",
      "|2041|   2021|\n",
      "|2041|   2020|\n",
      "|2041|   2045|\n",
      "+----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in [1e3, 2e3, 4e3]: #, 8e3, 1e4, 2e4, 4e4, 8e4]:\n",
    "    edges = timed_join(int(n))\n",
    "edges.cache()\n",
    "edges.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# qry = \"SELECT df1._c0 as id, df1.suburb_loc as suburb, df1.Combustibl as combustible, \" +\\\n",
    "#     \"df2._c0 as near_id, st_distance(st_point(df1.X, df1.Y), st_point(df2.X, df2.Y)) as distance \" +\\\n",
    "#     \"FROM df as df1, df as df2 \" +\\\n",
    "#     \"WHERE st_distance(st_point(df1.X, df1.Y), st_point(df2.X, df2.Y)) < 50\"\n",
    "# result = spark.sql(qry)\n",
    "# # print('ratio', result.count()/(df.count() **2))\n",
    "# # result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from https://github.com/locationtech/rasterframes/blob/43bd3b37f0b2931470b2d5b5757474d3c885e659/pyrasterframes/src/main/python/pyrasterframes/rasterfunctions.py#L718-L1062\n",
    "\n",
    "# from __future__ import absolute_import\n",
    "# from pyspark.sql.column import Column, _to_java_column\n",
    "# from pyspark.sql.functions import lit\n",
    "# # from .rf_context import RFContext\n",
    "# # from .rf_types import CellType, Extent, CRS\n",
    "\n",
    "# def _apply_column_function(name, *args):\n",
    "# #     jfcn = RFContext.active().lookup(name)\n",
    "#     jcols = [_to_java_column(arg) for arg in args]\n",
    "#     return Column(*jcols)\n",
    "\n",
    "# def st_polygonFromText(*args):\n",
    "#     \"\"\"\"\"\"\n",
    "#     return _apply_column_function('st_polygonFromText', *args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyrasterframes.utils import create_rf_spark_session\n",
    "# from pyrasterframes.rasterfunctions import *\n",
    "# # spark = create_rf_spark_session()\n",
    "# # st_polygonFromText\n",
    "# import pyspark.sql.functions as fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import geomesa_pyspark as gpk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(gpk.spark.GeoMesaSpark.apply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyContext(object):\n",
    "    \"\"\"\n",
    "    Entrypoint to RasterFrames services\n",
    "    \"\"\"\n",
    "    def __init__(self, spark_session):\n",
    "        self._spark_session = spark_session\n",
    "        self._gateway = spark_session.sparkContext._gateway\n",
    "        self._jvm = self._gateway.jvm\n",
    "        #jsess = self._spark_session._jsparkSession\n",
    "        #self._jrfctx = self._jvm.org.locationtech.rasterframes.py.PyRFContext(jsess)\n",
    "        self.context = self._jvm.org.locationtech.geomesa.spark.jts.DataFrameFunctions\n",
    "        \n",
    "    def lookup(self, function_name: str):\n",
    "        return getattr(self._jrfctx, function_name)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "join + count 1000 took 0:00:01.136435 with 4966 edges\n",
    "join + count 2000 took 0:00:01.729888 with 16148 edges\n",
    "join + count 4000 took 0:00:04.051351 with 41052 edges\n",
    "join + count 8000 took 0:00:14.923895 with 94758 edges\n",
    "join + count 10000 took 0:00:25.314117 with 124714 edges\n",
    "join + count 20000 took 0:01:34.159677 with 260362 edges\n",
    "join + count 40000 took 0:06:10.692259 with 517040 edges\n",
    "join + count 80000 took 0:20:28.373289 with 992991 edges\n",
    "\n",
    "###\n",
    "join + count 1000 took 0:00:00.871192 with 4966 edges\n",
    "join + count 2000 took 0:00:01.468635 with 16148 edges\n",
    "join + count 4000 took 0:00:03.582747 with 41052 edges\n",
    "join + count 8000 took 0:00:13.629605 with 94758 edges\n",
    "join + count 10000 took 0:00:24.058779 with 124714 edges"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
