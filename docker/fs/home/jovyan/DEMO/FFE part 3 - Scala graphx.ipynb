{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark2 = org.apache.spark.sql.SparkSession@f7b54e7\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@f7b54e7"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.locationtech.jts.geom._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.locationtech.geomesa.spark.jts._\n",
    "\n",
    "// import spark.implicits._\n",
    "// below hack\n",
    "val spark2: SparkSession = spark\n",
    "import spark2.implicits._\n",
    "spark.withJTS\n",
    "\n",
    "import org.apache.spark.graphx._\n",
    "// To make some of the examples work we will also need RDD\n",
    "import org.apache.spark.rdd.RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MAX_VERTICES = 1000\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val MAX_VERTICES = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "edges = [id: int, near_id: int]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3966"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val edges = spark2.read.load(\"file:///geodata/ffe_%1$s_edges.parquet\" format (MAX_VERTICES))\n",
    "edges.cache()\n",
    "edges.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "edge_rdd = MapPartitionsRDD[17] at map at <console>:47\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Edge(959,961,null)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// vertices.rdd.first()\n",
    "val edge_rdd: RDD[Edge[Any]] = \n",
    "    edges.rdd.map(x => Edge(x.getInt(0), x.getInt(1), null))\n",
    "edge_rdd.first()                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vertices = [_c0: int, TARGET_FID: int ... 15 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val vertices = spark2.read.load(\"file:///geodata/ffe_%1$s_vertices.parquet\" format (MAX_VERTICES))\n",
    "vertices.cache()\n",
    "vertices.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vert_rdd = MapPartitionsRDD[36] at map at <console>:47\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(0,(1,Houghton Bay,POLYGON ((1749551.504199982 5422104.252850056, 1749551.544950008 5422110.366449833, 1749556.703549862 5422110.332049847, 1749556.662749767 5422104.218400002, 1749551.504199982 5422104.252850056)),null))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// vertices.rdd.first()\n",
    "val vert_rdd: RDD[(VertexId, (Int, String, Any, Any))] = \n",
    "    vertices.rdd.map(x => (x.getInt(0), (x.getInt(1), x.getString(2), x.get(9), null)))\n",
    "vert_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "graph = org.apache.spark.graphx.impl.GraphImpl@1ec7fc0e\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.graphx.impl.GraphImpl@1ec7fc0e"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Build the initial Graph\n",
    "val graph = Graph(vert_rdd, edge_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "facts = MapPartitionsRDD[54] at map at <console>:46\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(28 is the null of 31, 74 is the null of 76, 210 is the null of 209, 223 is the null of 224, 248 is the null of 249, 248 is the null of 250, 320 is the null of 322, 404 is the null of 408, 445 is the null of 444, 535 is the null of 543, 550 is the null of 716, 550 is the null of 718, 550 is the null of 719, 550 is the null of 721, 550 is the null of 722, 550 is the null of 723, 550 is the null of 724, 550 is the null of 725, 550 is the null of 726, 550 is the null of 727, 550 is the null of 728, 550 is the null of 732, 550 is the null of 739, 550 is the null of 740, 550 is the null of 741, 551 is the null of 552, 551 is the null of 988, 551 is the null of 989, 551 is the nul..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val facts: RDD[String] =\n",
    "  graph.triplets.map(triplet =>\n",
    "    triplet.srcAttr._1 + \" is the \" + triplet.attr + \" of \" + triplet.dstAttr._1)\n",
    "facts.take(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3966"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.numEdges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(772,4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.inDegrees.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array((451,0), (454,0), (147,0), (155,0), (772,5), (752,64), (586,41), (667,3), (428,0), (464,0))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.triangleCount().vertices.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WIP playing with the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sourceId = 464\n",
       "initialGraph = org.apache.spark.graphx.impl.GraphImpl@6dc231f0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.graphx.impl.GraphImpl@6dc231f0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sourceId: VertexId = graph.vertices.take(10)(9)._1 // just some random vertex\n",
    "// Initialize the graph such that all vertices except the root have distance infinity.\n",
    "val initialGraph = graph.mapVertices((id, _) =>\n",
    "    if (id == sourceId) 0.0 else Double.PositiveInfinity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "464"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.vertices.take(10)(9)._1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "facts = MapPartitionsRDD[120] at map at <console>:48\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(26:Infinity --> 29:Infinity, 72:Infinity --> 74:Infinity, 202:Infinity --> 201:Infinity, 215:Infinity --> 216:Infinity, 240:Infinity --> 241:Infinity, 240:Infinity --> 242:Infinity, 312:Infinity --> 314:Infinity, 395:Infinity --> 399:Infinity, 434:Infinity --> 433:Infinity, 524:Infinity --> 532:Infinity, 538:Infinity --> 704:Infinity, 538:Infinity --> 706:Infinity, 538:Infinity --> 707:Infinity, 538:Infinity --> 709:Infinity, 538:Infinity --> 710:Infinity, 538:Infinity --> 711:Infinity, 538:Infinity --> 712:Infinity, 538:Infinity --> 713:Infinity, 538:Infinity --> 714:Infinity, 538:Infinity --> 715:Infinity, 538:Infinity --> 716:Infinity, 538:Infinity --> 720:Infinity, 538..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val facts: RDD[String] =\n",
    "  initialGraph.triplets.map(triplet =>\n",
    "    triplet.srcId + \":\" + triplet.srcAttr + \" --> \" + triplet.dstId + \":\" + triplet.dstAttr)\n",
    "facts.take(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GeoMesa Spark  - Scala",
   "language": "scala",
   "name": "geomesa_spark__scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
