{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FFE Part 2 geo-join \n",
    "\n",
    "This geo-join operation creates a set of edges for buildings where the distance between centers < N. \n",
    "\n",
    "Here, we take the wellington buildings data set prepare the data for stage 3  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "import geomesa_pyspark\n",
    "\n",
    "from pyspark.sql import Row\n",
    "conf = geomesa_pyspark.configure().setAppName('DemoPart2')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate() #.enableHiveSupport()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# geomesa_pyspark.init_sql(spark) in later version 2.4 DOCS)\n",
    "spark._jvm.org.apache.spark.sql.SQLTypes.init(spark._jwrapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CONFIG\n",
    "MAX_VERTICES = int(1e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"file:///home/jovyan/DEMO/buildings_raw.csv\"\n",
    "\n",
    "df = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export asset dataset to parquet\n",
    "\n",
    "The parquet format is richer thn CSV as it includes schema and partitioning info need be Spark jobs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+\n",
      "|_c0|            location|            geometry|            centroid|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "|  0|POINT (1749554.10...|POLYGON ((1749551...|POINT (1749554.10...|\n",
      "|  1|POINT (1749846.30...|POLYGON ((1749839...|POINT (1749846.30...|\n",
      "|  2|POINT (1749627.52...|POLYGON ((1749627...|POINT (1749627.52...|\n",
      "|  3|POINT (1749693.26...|POLYGON ((1749690...|POINT (1749693.26...|\n",
      "|  4|POINT (1749246.20...|POLYGON ((1749245...|POINT (1749246.20...|\n",
      "|  5|POINT (1749267.81...|POLYGON ((1749262...|POINT (1749267.81...|\n",
      "|  6|POINT (1750098.41...|POLYGON ((1750098...|POINT (1750098.41...|\n",
      "|  7|POINT (1750297.87...|POLYGON ((1750296...|POINT (1750297.87...|\n",
      "|  8|POINT (1749993.53...|POLYGON ((1749995...|POINT (1749993.53...|\n",
      "|  9|POINT (1752319.54...|POLYGON ((1752320...|POINT (1752319.54...|\n",
      "| 10|POINT (1751689.81...|POLYGON ((1751683...|POINT (1751689.81...|\n",
      "| 11|POINT (1749077.53...|POLYGON ((1749073...|POINT (1749077.53...|\n",
      "| 12|POINT (1748772.95...|POLYGON ((1748766...|POINT (1748772.95...|\n",
      "| 13|POINT (1748443.70...|POLYGON ((1748436...|POINT (1748443.70...|\n",
      "| 14|POINT (1748915.31...|POLYGON ((1748907...|POINT (1748915.31...|\n",
      "| 15|POINT (1748914.65...|POLYGON ((1748911...|POINT (1748914.65...|\n",
      "| 16|POINT (1749113.07...|POLYGON ((1749113...|POINT (1749113.07...|\n",
      "| 17|POINT (1749275.84...|POLYGON ((1749266...|POINT (1749275.84...|\n",
      "| 18|POINT (1749193.83...|POLYGON ((1749192...|POINT (1749193.83...|\n",
      "| 19|POINT (1749179.60...|POLYGON ((1749184...|POINT (1749179.60...|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 10 ms, sys: 0 ns, total: 10 ms\n",
      "Wall time: 3.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "qry = \"SELECT df.*, st_point(df.X, df.Y) AS location \" +\\\n",
    "    \"FROM df where df._c0 < %s\" % MAX_VERTICES\n",
    "\n",
    "result = spark.sql(qry)\n",
    "result.cache()\n",
    "result.write.save(\"file:///geodata/ffe_%s_vertices.parquet\" % MAX_VERTICES, format=\"parquet\", mode=\"overwrite\")\n",
    "result.select('_c0', 'location', 'geometry', 'centroid').show()\n",
    "#resultDataFrame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## repartition makes spark more efficient, more smaller task makes the workload smoother.\n",
    "\n",
    "newdf = df.repartition(72)\n",
    "newdf.createOrReplaceTempView('df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geo join => beating O(n^2)\n",
    "\n",
    "In pyspark we must use the SQL approach to access the geomesa geo functions (st_point, st_distance).\n",
    "\n",
    "Note that there's no performance differnce SQL and JDT API's - they both handled identically in Spark.Core."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join + count 1000 took 0:00:00.682460 with 1000 edges\n",
      "CPU times: user 0 ns, sys: 10 ms, total: 10 ms\n",
      "Wall time: 683 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "t0 = dt.utcnow()\n",
    "qry = \"SELECT df1._c0 as id, \" +\\\n",
    "    \"df2._c0 as near_id \" +\\\n",
    "    \"FROM df as df1, df as df2 \" +\\\n",
    "    \"WHERE st_distance(st_point(df1.X, df1.Y), st_point(df2.X, df2.Y)) < 50 \"+\\\n",
    "    \"AND df1._c0 <> df2._c0 \" +\\\n",
    "    \"AND df1._c0 < %s and df2._c0 < %s\" % (MAX_VERTICES, MAX_VERTICES)\n",
    "edges = spark.sql(qry)\n",
    "count = result.count()\n",
    "print(\"join + count %s took %s with %s edges\" % (MAX_VERTICES, dt.utcnow()-t0, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export edges dataset to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## export as parquet\n",
    "edges.write.save(\"file:///geodata/ffe_%s_edges.parquet\" % MAX_VERTICES, format=\"parquet\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
